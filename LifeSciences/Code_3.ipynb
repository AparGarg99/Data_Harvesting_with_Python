{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ WEB SCRAPING MODULES ############\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import bs4\n",
    "from fake_useragent import UserAgent\n",
    "import requests\n",
    "################ TIME MODLULES ###################\n",
    "import time\n",
    "from datetime import date \n",
    "import datetime\n",
    "############## DATA MANIPULATION MODULES #########\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import statistics \n",
    "from statistics import mode "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_and_text():\n",
    "    link=[i.get('href') for i in soup.find_all('a',{'style':'text-decoration:none;display:block'})]\n",
    "    text=[i.text for i in soup.find_all('div',{'class':'JheGif nDgy9d'})]\n",
    "    date=[i.text for i in soup.find_all('span',{'class':'WG9SHc'})]\n",
    "    website=[i.text for i in soup.find_all('div',{'class':'XTjFC WF4CUc'})]\n",
    "    ##################### FILTER VALUES ########################################\n",
    "    INDEX_1=[i for i in range(len(text)) if '$' in text[i]]\n",
    "    link=[link[i] for i in INDEX_1]\n",
    "    text=[text[i] for i in INDEX_1]\n",
    "    date=[date[i] for i in INDEX_1]\n",
    "    website=[website[i] for i in INDEX_1]\n",
    "    ############################################################################\n",
    "    return link,text,date,website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_amount(z):\n",
    "    key=['million','billion','millions','billions','trillion','trillions']\n",
    "    amount=[]\n",
    "    for x in z:\n",
    "        try:\n",
    "            if(x[x.index('$'):].split()[1].lower() in key):\n",
    "                amount.append(x[x.index('$'):].split()[0]+' '+x[x.index('$'):].split()[1])\n",
    "            else:\n",
    "                amount.append(x[x.index('$'):].split()[0])\n",
    "        except:\n",
    "            amount.append(x[x.index('$'):].split()[0])\n",
    "    return amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_non_public(company):\n",
    "    ticker=[]\n",
    "    listed=[]\n",
    "    public=[]\n",
    "    for i in company:\n",
    "        try:\n",
    "            driver.get('https://www.google.com/search?tbm=fin&q=NASDAQ%3A%20{}'.format(\"+\".join(i.split())))\n",
    "            source = driver.page_source\n",
    "            soup=bs4.BeautifulSoup(source, 'html.parser')\n",
    "            ticker.append(soup.find('span',{'jsname':'qRSVye'}).text[0])\n",
    "            listed.append(soup.find('div',{'class':'wx62f PZPZlf'}).text.split(':')[0])\n",
    "            public.append(1)\n",
    "        except:\n",
    "            time.sleep(5)\n",
    "            try:\n",
    "                driver.get('https://www.google.com/search?tbm=fin&q={}'.format(\"+\".join(i.split())))\n",
    "                source = driver.page_source\n",
    "                soup=bs4.BeautifulSoup(source, 'html.parser')\n",
    "                ticker.append(soup.find('span',{'jsname':'qRSVye'}).text[0])\n",
    "                listed.append(soup.find('div',{'class':'wx62f PZPZlf'}).text.split(':')[0])\n",
    "                public.append(1)\n",
    "            except:\n",
    "                ticker.append(np.nan)\n",
    "                listed.append(np.nan)\n",
    "                public.append(0)\n",
    "        time.sleep(5)\n",
    "    return public,listed,ticker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "user=input('Do you want to enter companies one by one?(y/n): ')\n",
    "l=[]\n",
    "if(user=='y'):\n",
    "    more='y'\n",
    "    while(more=='y'):\n",
    "        a=input('enter company name: ')\n",
    "        l.append(a)\n",
    "        more=input('do you want to enter more companies(y/n): ')\n",
    "else:\n",
    "    csv=input('enter name of excel file: ')\n",
    "    companies=input('enter column containing list of companies: ')\n",
    "    try:\n",
    "        l=list(pd.read_csv(csv)[companies])\n",
    "    except:\n",
    "        l=list(pd.read_excel(csv)[companies])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bisichem',\n",
       " 'Sinovac Biotech Co. Ltd.',\n",
       " 'FLX Bio, Inc.',\n",
       " 'Confo Therapeutics',\n",
       " 'NeuroCycle Therapeutics, Inc.',\n",
       " 'Protera ',\n",
       " 'Swiss Medical Union SA',\n",
       " 'Positrigo AG',\n",
       " 'Aspect Biosystems',\n",
       " 'deepCDR Biologics AG']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taking 10/424 companies for example\n",
    "l=l[:10]\n",
    "l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape [Google News](https://news.google.com/topstories?hl=en-IN&gl=IN&ceid=IN:en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bisichem\n",
      "#############################################\n",
      "Page No. 1\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 2\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 3\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 4\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 5\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "****************************************************************************************************\n",
      "Sinovac Biotech Co. Ltd.\n",
      "#############################################\n",
      "Page No. 1\n",
      "No. of links and text are:  2 2 2 2\n",
      "**************************************************\n",
      "Page No. 2\n",
      "No. of links and text are:  2 2 2 2\n",
      "**************************************************\n",
      "Page No. 3\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 4\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 5\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "****************************************************************************************************\n",
      "FLX Bio, Inc.\n",
      "#############################################\n",
      "Page No. 1\n",
      "No. of links and text are:  3 3 3 3\n",
      "**************************************************\n",
      "Page No. 2\n",
      "No. of links and text are:  6 6 6 6\n",
      "**************************************************\n",
      "Page No. 3\n",
      "No. of links and text are:  3 3 3 3\n",
      "**************************************************\n",
      "Page No. 4\n",
      "No. of links and text are:  2 2 2 2\n",
      "**************************************************\n",
      "Page No. 5\n",
      "No. of links and text are:  4 4 4 4\n",
      "**************************************************\n",
      "****************************************************************************************************\n",
      "Confo Therapeutics\n",
      "#############################################\n",
      "Page No. 1\n",
      "No. of links and text are:  1 1 1 1\n",
      "**************************************************\n",
      "Page No. 2\n",
      "No. of links and text are:  2 2 2 2\n",
      "**************************************************\n",
      "Page No. 3\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 4\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 5\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "****************************************************************************************************\n",
      "NeuroCycle Therapeutics, Inc.\n",
      "#############################################\n",
      "Page No. 1\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 2\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 3\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 4\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 5\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "****************************************************************************************************\n",
      "Protera \n",
      "#############################################\n",
      "Page No. 1\n",
      "No. of links and text are:  4 4 4 4\n",
      "**************************************************\n",
      "Page No. 2\n",
      "No. of links and text are:  3 3 3 3\n",
      "**************************************************\n",
      "Page No. 3\n",
      "No. of links and text are:  5 5 5 5\n",
      "**************************************************\n",
      "Page No. 4\n",
      "No. of links and text are:  4 4 4 4\n",
      "**************************************************\n",
      "Page No. 5\n",
      "No. of links and text are:  2 2 2 2\n",
      "**************************************************\n",
      "****************************************************************************************************\n",
      "Swiss Medical Union SA\n",
      "#############################################\n",
      "Page No. 1\n",
      "No. of links and text are:  3 3 3 3\n",
      "**************************************************\n",
      "Page No. 2\n",
      "No. of links and text are:  1 1 1 1\n",
      "**************************************************\n",
      "Page No. 3\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 4\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 5\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "****************************************************************************************************\n",
      "Positrigo AG\n",
      "#############################################\n",
      "Page No. 1\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 2\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 3\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 4\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 5\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "****************************************************************************************************\n",
      "Aspect Biosystems\n",
      "#############################################\n",
      "Page No. 1\n",
      "No. of links and text are:  7 7 7 7\n",
      "**************************************************\n",
      "Page No. 2\n",
      "No. of links and text are:  2 2 2 2\n",
      "**************************************************\n",
      "Page No. 3\n",
      "No. of links and text are:  1 1 1 1\n",
      "**************************************************\n",
      "Page No. 4\n",
      "No. of links and text are:  2 2 2 2\n",
      "**************************************************\n",
      "Page No. 5\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "****************************************************************************************************\n",
      "deepCDR Biologics AG\n",
      "#############################################\n",
      "Page No. 1\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 2\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 3\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 4\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 5\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "x,k,y,z,m=[],[],[],[],[]\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install()) # open Chrome driver/window\n",
    "for i in l:\n",
    "    print(i) # print company being scraped\n",
    "    print('#'*45)\n",
    "    # Scrape first 5 pages of \"<COMPANY NAME> raises financing\"\n",
    "    for j in range(0,50,10): \n",
    "        driver.get('https://www.google.com/search?&tbm=nws&q={}+raises+financing&start={}'.format(\"+\".join(i.split()),j))\n",
    "        source = driver.page_source\n",
    "        soup=bs4.BeautifulSoup(source, 'html.parser')\n",
    "        link,text,date,website=get_links_and_text()\n",
    "        print('Page No.',int(str(j)[0])+1)\n",
    "        print('No. of links and text are: ',len(link),len(text),len(date),len(website))\n",
    "        x.append(date)\n",
    "        y.append(link)\n",
    "        z.append(text)\n",
    "        k.append(website)\n",
    "        m.append([i for j in range(len(website))])\n",
    "        time.sleep(5)\n",
    "        print('*'*50)\n",
    "    print('*'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "x=[j for i in x for j in i]\n",
    "k=[j for i in k for j in i]\n",
    "y=[j for i in y for j in i]\n",
    "z=[j for i in z for j in i]\n",
    "m=[j for i in m for j in i]\n",
    "# Get amount from title\n",
    "amount=scrape_amount(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame from scraped features\n",
    "df=pd.DataFrame(zip(m,x,z,y,amount),\n",
    "                columns=['Company Name','Date','Article Title','URL','Amount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59, 5)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only those rows where Company Name is in Article Title\n",
    "df['temp']=df['Company Name'].apply(lambda x: x.split()[0])\n",
    "df['C'] = df.apply(lambda x: str(x['temp']) in str(x['Article Title']), axis=1)\n",
    "df=df.loc[df['C']==True].drop(['temp','C'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 0\n",
      "Article 1\n",
      "Article 2\n",
      "Article 3\n",
      "Article 4\n"
     ]
    }
   ],
   "source": [
    "# Parse each article in DataFrame and scrape investors and series type\n",
    "investors=[]\n",
    "series=[]\n",
    "for j,url in enumerate(df['URL']):\n",
    "    print('Article',j)\n",
    "    driver.get(url)\n",
    "    source = driver.page_source\n",
    "    soup=bs4.BeautifulSoup(source, 'html.parser')\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()    \n",
    "    a = soup.get_text()\n",
    "    a=\" \".join(a.split())\n",
    "    a=re.sub('[^A-Za-z0-9.]+', ' ', a)\n",
    "    investors.append([i for i in a.split('.') if bool(re.search('investors ',i.lower()))==True])\n",
    "    series.append([i.lower()[i.lower().find('series'):i.lower().find('series')+8] for i in a.split('.') if i.lower().find('series')!=-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize investors and series type lists\n",
    "series2=[]\n",
    "def most_frequent(List): \n",
    "    return max(set(List), key = List.count) \n",
    "\n",
    "for i in range(len(investors)):\n",
    "    if(investors[i]==[]):\n",
    "        investors[i]=np.nan\n",
    "\n",
    "for i in series:\n",
    "    try:\n",
    "        series2.append(most_frequent(i))\n",
    "    except:\n",
    "        series2.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save lists to DataFrame\n",
    "df['Series Type']=series2\n",
    "df['Investors']=investors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all values NaN in column \"Series Type\", except those which are in list allowed_vals\n",
    "allowed_vals=['series b', 'series d', 'series a', 'series c','series e']\n",
    "df['Series Type'][~df['Series Type'].isin(allowed_vals)] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arranging Columns of DataFrame\n",
    "df=df[['Company Name', 'Date', 'Article Title', 'URL', 'Series Type', 'Amount',\n",
    "       'Investors']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 7)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify Date Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want for a date range(y/n): n\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    d_range=input('Do you want for a date range(y/n): ')\n",
    "    if(d_range=='y'):\n",
    "        date1 = input('From(YYYY-MM-DD): ')\n",
    "        date2 = input('To(YYYY-MM-DD): ')\n",
    "        dates=[]\n",
    "        dates2=[]\n",
    "        start = datetime.datetime.strptime(date1, '%Y-%m-%d')\n",
    "        end = datetime.datetime.strptime(date2, '%Y-%m-%d')\n",
    "        step = datetime.timedelta(days=1)\n",
    "    ################################################################\n",
    "        while start <= end:\n",
    "            dates.append (str(start.date()))\n",
    "            start += step\n",
    "        for i in dates:\n",
    "            a1=i.split('-')\n",
    "            x = datetime.datetime(int(a1[0]),int(a1[1]),int(a1[2]))\n",
    "            dates2.append(x.strftime(\"%b %d, %Y\"))\n",
    "    ##################################################################\n",
    "        df=df.loc[df['Date'].isin(dates2)]\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding not-scraped companies to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=df['Company Name'].unique()\n",
    "not_scraped=list(set(l)-set(k))\n",
    "for i in not_scraped:\n",
    "    s2 = pd.DataFrame([i,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan]).T\n",
    "    s2.columns=df.columns\n",
    "    df=df.append(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 7)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sheet 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amit\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\Amit\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df2=df.groupby(['Company Name']).size().reset_index(name='Value')\n",
    "df2['Financing Rounds (Y/N)']=['Y' for i in range(df2.shape[0])]\n",
    "for i in range(df2.shape[0]):\n",
    "    if(df2['Company Name'][i] in not_scraped):\n",
    "        df2['Value'][i]=0\n",
    "        df2['Financing Rounds (Y/N)'][i]='N'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Value</th>\n",
       "      <th>Financing Rounds (Y/N)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Aspect Biosystems</td>\n",
       "      <td>2</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Bisichem</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Confo Therapeutics</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>FLX Bio, Inc.</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>NeuroCycle Therapeutics, Inc.</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Positrigo AG</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Protera</td>\n",
       "      <td>1</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Sinovac Biotech Co. Ltd.</td>\n",
       "      <td>2</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Swiss Medical Union SA</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>deepCDR Biologics AG</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Company Name  Value Financing Rounds (Y/N)\n",
       "0              Aspect Biosystems      2                      Y\n",
       "1                       Bisichem      0                      N\n",
       "2             Confo Therapeutics      0                      N\n",
       "3                  FLX Bio, Inc.      0                      N\n",
       "4  NeuroCycle Therapeutics, Inc.      0                      N\n",
       "5                   Positrigo AG      0                      N\n",
       "6                       Protera       1                      Y\n",
       "7       Sinovac Biotech Co. Ltd.      2                      Y\n",
       "8         Swiss Medical Union SA      0                      N\n",
       "9           deepCDR Biologics AG      0                      N"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "name='BiosectRx Financing Harvest-'+str(date.today())+'.xlsx'\n",
    "writer = pd.ExcelWriter(name, engine='xlsxwriter')\n",
    "df2.to_excel(writer, sheet_name='Financing Check',index=False)\n",
    "df.to_excel(writer, sheet_name='Harvest',index=False)\n",
    "writer.save()\n",
    "driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
